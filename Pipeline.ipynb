{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["nPqJQ0OinYWw"],"mount_file_id":"1XbH4cs8vhySuoIycSR_nhQUFZE2c1caf","authorship_tag":"ABX9TyNr6yTN35M+BBJLJcdw048V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vtIPMrsJEFOg"},"outputs":[],"source":["cd yourlocation/Linking_Judgements/"]},{"cell_type":"markdown","source":["# **Preprocessing and segmenting judgement and timestamps**"],"metadata":{"id":"vg6ZvSKs395B"}},{"cell_type":"code","source":["import cleanJudgement as cj\n","import segementJudgement as sg\n","import keywordExtraction as key"],"metadata":{"id":"C9wg0xQP-yp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cleaning the judgement\n","judge= cj.cleanj(file of judgement in txt format with integered paragraphs)\n","clean_judge = judge.preprocessDoc()\n","# print(clean_judge)"],"metadata":{"id":"MmYqpq8TEwdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Option 1: segment judgement with paragraphs\n","\n","judge_all = sg.segmentJ(clean_judge, paragraphs=True)\n","paragraphs = judge_all.paras() #list of paragraphs use for entailement\n","# get the length of the paragraphs\n","lengths_paragraphs = [len(t.split()) for t in paragraphs]"],"metadata":{"id":"HV1I-GfwMr3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Option2: segment judgement with hard-coded sections\n","# section_names = ['Summary','The background','The facts of this case','Preserving the status quo','Conclusions in principle',' The Outcome in this Case']\n","# judge_all = sg.segmentJ(clean_judge, paragraphs=False)\n","# secs = judge_all.sections(section_names)\n","# summary = secs[0]\n","# # text only summary\n","# text_summary = ''.join(secs[0])  #NEEDED FOR EMBEDDING AND JSON FILE\n","# text_summary"],"metadata":{"id":"UVLws2u-3uLT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cut first two lines if they are dates (check first paragaphs[0])\n","paragraphs = paragraphs[1:]\n","paragraphs"],"metadata":{"id":"whkwIZofZFTK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Creating  windows of paragraphs**"],"metadata":{"id":"oSR3oJUOsv9b"}},{"cell_type":"code","source":["# Get average token length for debugging GPT 3 and other models\n","lengths_paragraphs = [len(t.split()) for t in paragraphs]\n","max(lengths_paragraphs)\n","from matplotlib import pyplot as plt\n","plt.hist(lengths_paragraphs, 10)\n","\n","plt.show()\n","                          "],"metadata":{"id":"Mz8EGZgqFafT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get average length\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","Average(lengths_paragraphs)\n"],"metadata":{"id":"pGasEeMjDqhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create segements of 3 consecutive paragraphs\n","chunks = [paragraphs[x:x+3] for x in range(0, len(paragraphs), 3)]\n","# connect paragraphs with a new line\n","window_paras_strings = [\"\\n \".join(p) for p in chunks]\n","#get lengths of segments\n","lengths_windows = [len(t.split()) for t in window_paras_strings]\n","print(max(lengths_windows))\n","print(min(lengths_windows))\n","print(Average(lengths_windows))"],"metadata":{"id":"Wpztlg4ZzBQO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Clean timestamps**"],"metadata":{"id":"WZkG0MUbKKCo"}},{"cell_type":"code","source":["#Clean and extract timestamps related to the summary section\n","\n","import timestampExraction as ts\n","\n","transcript = ts.timestamp(txt file for collated csv files for sessions)\n","clean_transcript = transcript.segment()\n","# print(clean_transcript)\n","# exclude very short timestamps and get timestamps list with timestamps with +50 tokens\n","long_timestamps = transcript.longTimestamps(clean_transcript)  # with timestamp[s]\n","# # print(len(long_timestamps))\n","\n","# # get the text only timestamps related to the summary section without the timespans\n","text_timestamp = transcript.getText(long_timestamps) #TIMESTAMPS WITHOUT TIMES\n","# print(text_timestamp_summary[0])"],"metadata":{"id":"fN-Y29ypQ9mc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Related to option 2 above: Extract links that has the same entities as the judgement paragraphs/sections\n","#extract both quoted keywords and BLACKSTONE NEs from judgement sections\n","# The Summary list as an example\n","# extractor = key.extractkeywords()\n","# summary_quotes = extractor.quotes_extract(secs[0])\n","# full_list_summary = extractor.create_NE_lists(csv file with entities,summary_quotes)\n","# full_list_summary\n","\n"," # get only the timetamps that has the summary keywords and entities\n","# summary_timestamps = transcript.extractTimestamps(long_timestamps,full_list_summary )  #TIMESTAMPS WITH TIME\n","# print(len(summary_timestamps))"],"metadata":{"id":"8MI0858s6S6A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get Embedding files for judgement paragraphs and timestamps"],"metadata":{"id":"nPqJQ0OinYWw"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"id":"CsklFGn8nhZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai"],"metadata":{"id":"Xrl63nJ2tIXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imports\n","import pandas as pd\n","import tiktoken\n","import openai\n","\n","from openai.embeddings_utils import get_embedding"],"metadata":{"id":"AoBVbVshrEQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We save the embeddigns of the paragraphs and the timestamps and then calculate the dot product"],"metadata":{"id":"5612bPRx1UPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedding model parameters\n","embedding_model = \"text-embedding-ada-002\"  # text embedding\n","embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n","max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191"],"metadata":{"id":"RdxK4PWDrLkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["api_key = 'your api key for OpenAI'\n","openai.api_key = api_key"],"metadata":{"id":"h2_KXLUqrsTp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Get judgement paragraph embeddings**"],"metadata":{"id":"3IcsGpeHotC-"}},{"cell_type":"code","source":["df_judge = pd.DataFrame(window_paras_strings, columns =['text'])\n","df_judge.head()"],"metadata":{"id":"LMbmwOLxpIpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get num of tokens per the model tokenizer\n","encoding = tiktoken.get_encoding(embedding_encoding)\n","df_judge[\"n_tokens\"] = df_judge.text.apply(lambda x: len(encoding.encode(x)))\n","print(df_judge['n_tokens'].max())"],"metadata":{"id":"fulldu24qOHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save GPT embeddings for future use to avoid paying every time you make an API request**"],"metadata":{"id":"DxM54b2u-bWw"}},{"cell_type":"code","source":["cd to an embedding forlder you create\n","!mkdir make a directory for each case"],"metadata":{"id":"uQcZ-jh-rNyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n","# Embeddings for paragraphs\n","# This may take a few minutes\n","df_judge[\"embedding\"] = df_judge.text.apply(lambda x: get_embedding(x, engine=embedding_model))\n","df_judge.to_csv(\"./name of directory above/name of the case.csv\")"],"metadata":{"id":"inDFf3Cgqn_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  it is better to save the embeddings in a pickle file to avoid problems of data value change\n","import pickle\n","df_judge.to_pickle(\"./name of directory above/name of the case.pkl\")"],"metadata":{"id":"Rf62sqCiM-JH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Get timestamps embeddings**"],"metadata":{"id":"quUcmSV4o1N4"}},{"cell_type":"code","source":["df_times = pd.DataFrame(text_timestamp, columns =['text'])\n","\n","# get num of tokens per the model tokenizer\n","df_times[\"n_tokens\"] = df_times.text.apply(lambda x: len(encoding.encode(x)))\n","print(df_times['n_tokens'].max())"],"metadata":{"id":"cCPQ68mZo9UO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add timestamps in the data frame\n","\n","times = [t[0] for t in long_timestamps]\n","\n","df_times['timestamps'] = times "],"metadata":{"id":"YrBQ7tm3tC_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embeddings for timestamps\n","# This may take a few minutes\n","df_times[\"embedding\"] = df_times.text.apply(lambda x: get_embedding(x, engine=embedding_model))\n","#debug to see that all timestamps has been embedded\n","df_times['embedding'].isnull().sum()\n"],"metadata":{"id":"9U71wEOao9US"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save embedding files for timestamps in csv and pkl\n","df_times.to_csv(\"./name of directory above/name of the case.csv\")\n","df_times.to_pickle(\"./name of directory above/name of the case.pkl\")"],"metadata":{"id":"gpntnIIVMecO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Calculate pairwise similarity between embeddings of paragraphs and timestamps**"],"metadata":{"id":"G3h8Fv5SrcLr"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"metadata":{"id":"xa1ET-RYPAeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd to the embedding folder where you have the embeddings stored"],"metadata":{"id":"cdymAYU_kMWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# open from pickle file\n","with (open('embeddings of timestamsps.pkl', \"rb\")) as openfile:\n","     df_times = pickle.load(openfile)\n","df_times.head()"],"metadata":{"id":"3D9sX33NOla_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#same as above cell but from CSV\n","\n","# get the embeddings of timestamps\n","# datafile_path = 'embeddings of timestamsps.pkl'\n","\n","# df_times = pd.read_csv(datafile_path)\n","\n"],"metadata":{"id":"lV-4NedXralh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a list of embeddings to get indices of timestamps\n","list_embeddings = df_times[\"embedding\"].to_list()"],"metadata":{"id":"EXRPDeJHz-Gu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# open file for judgemnt embeddings\n","\n","with (open('file for judgement embeddings.pkl', \"rb\")) as openfile:\n","     df_judge = pickle.load(openfile)\n","df_judge.head()"],"metadata":{"id":"irD33QvZPKWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# same as above but with CSV\n","\n","datafile_path = \"file for judgement embeddings.csv\"\n","\n","df_judge = pd.read_csv(datafile_path)\n","df_judge[\"embedding\"] = df_judge.embedding.apply(eval).apply(np.array)"],"metadata":{"id":"SYJO-rA-ZuzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai.embeddings_utils import cosine_similarity\n","\n","# caluculate the similarity of each judgement segment with each embedding for a timestamp in the list of timestamps and add index of timestamp\n","\n","\n","CS = []\n","for i in range(len(df_judge['embedding'])):\n","  temp = []\n","  for n in range(len(list_embeddings)):\n","    temp.append((cosine_similarity(df_judge['embedding'][i],list_embeddings[n]),n))\n","  temp = sorted(temp, reverse=True)\n","  # print(temp)\n","  CS.append(temp)\n","\n"],"metadata":{"id":"iqDSIIMDGJVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save cosine similarities for future use**"],"metadata":{"id":"8kT6PyqSAWPv"}},{"cell_type":"code","source":["import csv\n","with open('cosinesimialrity.csv','w',newline='') as out:\n","    csv_out=csv.writer(out)\n","    csv_out.writerow(['cosine_similarity','index'])\n","    for row in CS:  \n","        csv_out.writerow(row)\n"],"metadata":{"id":"d_byHZCWoPyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Create Json file without keyword tags**"],"metadata":{"id":"kRZhdDyhptgC"}},{"cell_type":"code","source":["# a function to take the cosine similarity (list of 20), the list of long_times, the text of the segment and the name of the segment and the id of the segment and returns a dictionary\n","\n","# def get_dics(cosine_sim,timestamps_ls, section,file_name, id):\n","#     times = []\n","#     texts = []\n","#     for t in cosine_sim:\n","#         text = timestamps_ls[t[1]][1]\n","#         time = timestamps_ls[t[1]][0]\n","#         times.append(time)\n","#         texts.append(text)\n","#         dictionary = {file_name:section}\n","#         dictionary['Transcription'] = [{'time': times_sp, 'text': trans_te} for times_sp, trans_te in zip(times, texts)]\n","\n","#     data_id= {'Segment_id': id}\n","#     data_id.update(dictionary)\n","#     return data_id"],"metadata":{"id":"oHwtR14ELs97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the same as above but working with the dfs\n","\n","def get_dics_withdf(cosine_sim,times_df, section,file_name, id):\n","    times = []\n","    texts = []\n","    for t in cosine_sim:\n","        text = times_df['text'][t[1]]\n","        time = times_df['timestamps'][t[1]]\n","        times.append(time)\n","        texts.append(text)\n","        dictionary = {file_name:section}\n","        dictionary['Transcription'] = [{'time': times_sp, 'text': trans_te} for times_sp, trans_te in zip(times, texts)]\n","\n","    data_id= {'Segment_id': id}\n","    data_id.update(dictionary)\n","    return data_id"],"metadata":{"id":"y7SDEIS1j936"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a dictionary of dictionaries for all the paragraph segments with the dfs\n","\n","dicts2 = []\n","id_ = 1\n","for c in CS2:\n","  ind = CS2.index(c)  \n","  dict_temp = get_dics_withdf(c,df_times, df_judge['text'][ind],'Segment', id_)\n","  dicts2.append(dict_temp)\n","  id_ += 1"],"metadata":{"id":"ZhzMhGhJs1Fx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Create Json file with keyword tags**"],"metadata":{"id":"2-JdCSDfaNyN"}},{"cell_type":"markdown","source":["**Add keyword tag to entities in judgement and timestamps**\n","\n"],"metadata":{"id":"erH7bXV3yEXv"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","entities = pd.read_csv(csv file with entities extracted from judgement, names=['word','label'])"],"metadata":{"id":"bOfcNGr1yDTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entities_list = entities['word'].to_list()"],"metadata":{"id":"YPN8QD9C0OhD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Clean the entities list. Use this function if the extractor module is not used in the option 2 above. You can uncomment or comment as many of the cleaning lines as needed**"],"metadata":{"id":"GX2L8TdRBia8"}},{"cell_type":"code","source":["import re\n","#clean list\n","def create_NE_lists(list_ents):\n","  # data = pd.read_csv(file, encoding='utf8')\n","  \n","  # printable = set(string.printable) #clean non-utf8\n","  # ents = [\"\".join(filter(lambda c: c in printable, ent)) for ent in ents]\n","  \n","  # new_ents = remove_stopwords(new_ents) #clean stopwords\n","  new_ents = [x for x in list_ents if x !='I agree'] #clean 'I agree'\n","  new_ents = [elem for elem in new_ents if len(elem) > 10] #excluse short NEs\n","  # new_ents = [re.sub(\"\\[\\d+\\]\\s+\" , \"\", ent) for ent in new_ents] #clean patterns like [2008] \n","  # new_ents = [re.sub(\"\\(\\d+\\)\\s*\" , \"\", ent) for ent in new_ents] #clean patterns like [2008] \n","  # new_ents = [re.sub(\"\\[\" , \"\", ent) for ent in new_ents] # clean brackets\n","  # new_ents = [re.sub(\"\\]\" , \"\", ent) for ent in new_ents] # clean brackets\n","  # new_ents = [re.sub(\"\\(\" , \" \", ent) for ent in new_ents] # clean brackets\n","  # new_ents = [re.sub(\"\\)\" , \" \", ent) for ent in new_ents] # clean brackets\n","  # new_ents = [re.sub('\"' , '', ent) for ent in new_ents] #clean quotations\n","  # ents =  [re.sub(\"\\s\\s+\" , \" \", ent) for ent in new_ents]  #clean extra spaces\n","  new_ents = list(dict.fromkeys(new_ents)) #delete duplicates \n","  # new_ents = [t.strip() for t in new_ents]\n","  # new_ents = new_ents+quot\n","  return new_ents"],"metadata":{"id":"vbvP7Jqdt7yP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**List without duplicates and short strings**"],"metadata":{"id":"jnIewHwcdXjo"}},{"cell_type":"code","source":["clean = create_NE_lists(entities_list)\n","clean "],"metadata":{"id":"kC3Rrv-ut8K8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#exclude partial match of longer entities (This is used only for creating a Json file for annotation.)\n","\n","result = [item for item in clean if sum(substr not in item for substr in clean)==len(clean)-1]"],"metadata":{"id":"zwuo8U89vxUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Escaping brackets for regex (again this is used to have the keywords in html format \\<keywords> needed for the annotation tool)**"],"metadata":{"id":"qIvXDgxcdrD1"}},{"cell_type":"code","source":["import re\n","cleanesc = [re.escape(T) for T in result]\n"],"metadata":{"id":"4U69nBJst8K-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Function to add tags \\<keyword\\>**"],"metadata":{"id":"4ZqbD48gdfBH"}},{"cell_type":"code","source":["#function to add keyword tags\n","import re\n","\n","def add_keyword_tag(ents_list,text):\n","    \"\"\" Method takes a list of NEs and a text and returns a tagged text:\n","    :param ent_list: list of entities (str)\n","    :param text: text judgement or transcript (str)\n","    \"\"\"\n","    KEYWORD_PRE = '<KEYWORD>'\n","    KEYWORD_PRE_LEN = len(KEYWORD_PRE)\n","    KEYWORD_POST = '</KEYWORD>'\n","    KEYWORD_POST_LEN = len(KEYWORD_POST)\n","    # idx = 0\n","    # inds = []\n","    for k in ents_list:\n","        idx = 0\n","        for match in re.finditer(k,text):\n","            text = text[:match.start()+idx] + KEYWORD_PRE + text[match.start()+idx:]\n","            idx += KEYWORD_PRE_LEN\n","            text = text[:match.end()+idx] + KEYWORD_POST + text[match.end()+idx:]\n","            idx += KEYWORD_POST_LEN\n","        \n","    return(text)"],"metadata":{"id":"FBRsM21Ot8K9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Add keywords to text in judement and timestamps df**"],"metadata":{"id":"svm7wlvyu1IG"}},{"cell_type":"code","source":["df_times['text'] = df_times['text'].apply(lambda x: add_keyword_tag(cleanesc,x))"],"metadata":{"id":"3FDa-Upw0er5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_judge['text'] = df_judge['text'].apply(lambda x: add_keyword_tag(cleanesc,x))"],"metadata":{"id":"nvbzQf57vjHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create dict for json file with keywords**"],"metadata":{"id":"20d_M3MKvs00"}},{"cell_type":"code","source":["def get_dics_withdf(cosine_sim,times_df, section,file_name, id):\n","    times = []\n","    texts = []\n","    for t in cosine_sim:\n","        text = times_df['text'][t[1]]\n","        time = times_df['timestamps'][t[1]]\n","        times.append(time)\n","        texts.append(text)\n","        dictionary = {file_name:section}\n","        dictionary['Transcription'] = [{'time': times_sp, 'text': trans_te} for times_sp, trans_te in zip(times, texts)]\n","\n","    data_id= {'Segment_id': id}\n","    data_id.update(dictionary)\n","    return data_id"],"metadata":{"id":"8p6VEx-UwEwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a dictionary of dictionaries for all the paragraph segments\n","\n","dicts2 = []\n","id_ = 1\n","for c in CS2:\n","  ind = CS2.index(c)  \n","  dict_temp = get_dics_withdf(c,df_times, df_judge['text'][ind],'Segment', id_)\n","  dicts2.append(dict_temp)\n","  id_ += 1"],"metadata":{"id":"358GC-zkwEwT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Add urls ( You need to get these from the following archive: https://discovery.nationalarchives.gov.uk/ )**"],"metadata":{"id":"0fPktgVTwEwT"}},{"cell_type":"code","source":["#Example of a case \n","# urls = ['https://ds-live-videos.s3.amazonaws.com/66/UKSC/1/cr2-09-11-23-session1_imx30_1.webm',\n","#         'https://ds-live-videos.s3.amazonaws.com/66/UKSC/1/cr2-09-11-23-session2_imx30_1.mp4',\n","#         'https://ds-live-videos.s3.amazonaws.com/66/UKSC/1/cr2-09-11-24-session1_imx30_1.mp4',\n","#         'https://ds-live-videos.s3.amazonaws.com/66/UKSC/1/cr2-09-11-24-session2_imx30_1.mp4']"],"metadata":{"id":"_HkyszvwwEwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add ursl\n","\n","data_url = {'URLS':urls}\n","dicts2.insert(0, data_url)"],"metadata":{"id":"Wy8XpCAwwEwU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Create Json file**"],"metadata":{"id":"zoP_lKoV-45_"}},{"cell_type":"code","source":["cd to your json file directory"],"metadata":{"id":"tz6FSM8_jaIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the json file with the dictionaries\n","import json\n","\n","with open(\"Nameofjson file\", \"w\") as final:\n","    json.dump(dicts2, final)"],"metadata":{"id":"p0KX7aTtl3fg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Alternative Similarity Models**\n","**Note**\n","1. $\\color{blue}{\\text{The ouptut is a Json file with paragraph and first 20 links (you can change the number of links based on the use case)}}$\n","2. $\\color{blue}{\\text{The functions are created with  a list of timestamps text ($\\color{green}{\\text{text_timestamp}}$ variable above) and one judgement section.  You need to forloop through the (($\\color{green}{\\text{window_paras_strings}}$ variable above) to calculate with paragraphs.}}$ \n","\n"],"metadata":{"id":"fE8WtH53LX79"}},{"cell_type":"markdown","source":["# **Document Similarity with tf-idf**\n","\n","\n","\n","\n","\n"],"metadata":{"id":"3f-cdtSy6Guz"}},{"cell_type":"code","source":["from featureExtraction.tfidf import tf_idf_similarity\n","import calculateSimilarity as cosine"],"metadata":{"id":"RLlF7-XjMey4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = tf_idf_similarity(text_summary,text_timestamp_summary)"],"metadata":{"id":"4G9GM7pYM_Uv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a json file with the top 20 similarities\n","\n","cosine.get_results(results,20,text_timestamp,summary_TEXT,'Summary_linking_tfidf' )"],"metadata":{"id":"POFAuUlgj8Ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Similarity with BM25**"],"metadata":{"id":"T6-ogB0AfTd-"}},{"cell_type":"code","source":["!pip install -U sentence-transformers rank_bm25"],"metadata":{"id":"pyhhTVGxfSTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from featureExtraction import BM25 as bm"],"metadata":{"id":"faI6gPG2DWri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = bm.bm25(text_timestamp,text_summary)"],"metadata":{"id":"fF6CDZYu_TEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from evaluationNumpy import get_results"],"metadata":{"id":"fkobnb1HD_1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a json file from results\n","get_results(test,20,text_timestamp,summary_TEXT,'summary_test')"],"metadata":{"id":"IbN8SrLVD_1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Document Similarity with Pooling (MEAN, MIN and Max Pooling of Glove Embeddings)**"],"metadata":{"id":"TmCXt2Zy8n8v"}},{"cell_type":"code","source":["!pip install flair"],"metadata":{"id":"PxQ6o8dL54nD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import featureExtraction.doc_pool as docp\n","import evaluationNumpy as eval"],"metadata":{"id":"hPt-u_mQBMmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = docp.embeddDoc(text_summary,text_timestamp_summary)"],"metadata":{"id":"5DzE8ojdp013"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# doesnot work with GPU\n","summary_embedding,timestamps_embedding = corpus.get_embeddings_mean()"],"metadata":{"id":"3bru819GqydA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_embedding,timestamps_embedding = corpus.get_embeddings_min()"],"metadata":{"id":"b4Yca5tt-muk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_embedding,timestamps_embedding = corpus.get_embeddings_max()"],"metadata":{"id":"mtAHdl9RAomQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get sorted similarities with indexes\n","similarity = calculate_cosine_similarity(timestamps_embedding,summary_embedding)"],"metadata":{"id":"K426tBCjB6yF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#write results in json file\n","eval.get_results(similarity,20,summary_timestamps,text_summary,'Summary_pooling_max')"],"metadata":{"id":"R2KZyMC7AKh1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Document Similarity with RNN Embeddings (last hidden layer)**"],"metadata":{"id":"BK6IePORS_fs"}},{"cell_type":"code","source":["import featureExtraction.doc_pool as docp"],"metadata":{"id":"S7wLmH0d5qh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_embedding,timestamps_embedding = corpus.get_embeddings_rnn()"],"metadata":{"id":"8HAZUSX253xt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluateTensors as evalt\n","cosine = evalt.calculate_cosine_similarity(timestamps_embedding,summary_embedding)"],"metadata":{"id":"1bHL-Qno6knH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a json file with the top 20 similarities\n","import json\n","evalt.get_results(cosine,20,summary_timestamps,text_summary,'Summary_linking_rnn' )"],"metadata":{"id":"eFVPQxaq2CSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Semantic Search using 'all-MiniLM-L6-v2' (Entailment Embeddings fine-tuned on Multi-NLI**"],"metadata":{"id":"Hpvf5lOrW3Bk"}},{"cell_type":"code","source":["!pip install -U sentence-transformers"],"metadata":{"id":"BvQdP0KcW3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from featureExtraction.sentenceTrasformer import getEmbeddings\n","from sentence_transformers import SentenceTransformer,util"],"metadata":{"id":"xV8Q2Qn6W3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity and results top 20 most relevant links\n","# a list of tuples (ind,score)\n","#similarity_method (util.cos_sim or util.dot_score)\n","\n","results = getEmbeddings('all-MiniLM-L6-v2',text_summary,text_timestamp_summary,util.cos_sim)"],"metadata":{"id":"SejxH4-dW3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from evaluationNumpy import get_results"],"metadata":{"id":"Gl6vXsutW3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a json file from results\n","get_results(results,20,summary_timestamps,text_summary,'summary_test')"],"metadata":{"id":"ydTHzGKOW3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Asymmetric Similarity with SBERT Sentence Embeddings with dot product**"],"metadata":{"id":"Ko_8vRCE0MJs"}},{"cell_type":"code","metadata":{"id":"kTdV-MdYchp7"},"source":["!pip install -U sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity and results top 20 most relevant links\n","# a list of tuples (ind,score)\n","#similarity_method (util.cos_sim or util.dot_score)\n","\n","results = getEmbeddings('msmarco-roberta-base-ance-firstp',text_summary,text_timestamp_summary,util.dot_score)"],"metadata":{"id":"hq7JKGw9FTUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a json file from results\n","get_results(results,20,summary_timestamps,text_summary,'xxxxxxxxxxxx')"],"metadata":{"id":"oGDzYu_hGTFm"},"execution_count":null,"outputs":[]}]}